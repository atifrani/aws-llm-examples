{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference: Generate titles for chapterized components from the processed data\n",
    "---\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. We are extracted the processed data, including the messages from speakers in transcripts\n",
    "\n",
    "2. Loading those into our bedrock models: ***Titan, Claude, Llama***\n",
    "\n",
    "3. Generate chapter titles for each chapter from respective meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import yaml\n",
    "import copy\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd  \n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from litellm import completion ## support for text generation models on bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set a logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the config file: Contains model information, data directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the config file\n",
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous function series to generate titles to chapters from models specified in the [`config file`](config.yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to encapsulate call to any Bedrock model for text generation\n",
    "def generate_chapter_title(model_id: str, prompt: str) -> Dict:\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name\n",
    "    logger.info(f\"model_id={model_id}, prompt length is {len(prompt)} characters, {len(prompt.split())} words\")      \n",
    "\n",
    "    # initialize the response dict\n",
    "    ret = dict(exception = None,\n",
    "               prompt = prompt,\n",
    "               completion = None,\n",
    "               file_name = None,\n",
    "               chapter_id = None, \n",
    "               model_id = model_id,\n",
    "               time_taken_in_seconds = None, \n",
    "               # initializing to 0 since none type throws an error later, this is used to calculate price per token input/output on ODT pricing\n",
    "               completion_token_count = 0,\n",
    "               # initializing to 0 since none type throws an error later\n",
    "               prompt_token_count=0,\n",
    "               input_token_price = None, \n",
    "               output_token_pricing = None, \n",
    "               chapter_text = None)\n",
    "    body = ret['prompt']\n",
    "    # set the env var for aws_region\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "\n",
    "    # get the inference parameters from the config file    \n",
    "    parameters = config['inference_parameters_for_title_generation']\n",
    "    temperature = parameters.get('temperature', 0.1)\n",
    "    caching = parameters.get('caching', False)\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{ \"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=temperature,\n",
    "                              caching=caching)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "                prefixes_to_remove = config['response_prefix_to_remove']\n",
    "                if prefixes_to_remove:\n",
    "                    for p in prefixes_to_remove:\n",
    "                        ret[\"completion\"] = ret[\"completion\"].replace(p, \"\")\n",
    "                response_suffix_to_clip = config['response_suffix_to_clip']\n",
    "                if response_suffix_to_clip:\n",
    "                    ret['completion'] = ret['completion'].split(response_suffix_to_clip)[0].strip()\n",
    "                logger.info(f\"idx={idx}, choice.message.content={choice.message.content}\")\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running invocations for all bedrock models to generate chapter titles for meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Represents all of the processed csv files to be used to generate titles to chapters from each\n",
    "processed_data_chapters_fpath = os.path.join(config['dir']['processed'], config['dir']['chapterized_file'])\n",
    "logger.info(f\"going to read data from {processed_data_chapters_fpath}\")\n",
    "# we want to run through all the rows of the processed dataframe for each model\n",
    "# that we want to evaluate. So we will run a loop to `apply` each model on each row\n",
    "df = pd.read_csv(processed_data_chapters_fpath)\n",
    "logger.info(f\"there are {len(df.file_name.unique())} {config['dir']['file_type_to_process']} files to process\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    #print(f\"row={row}\")\n",
    "    logger.info(f\"row {i}/{total}, prompt_template={model_info['prompt_template']}, model_id={model_info['model']}\")\n",
    "    model_id = model_info['model']\n",
    "    fpath = os.path.join(config['dir']['prompts'], model_info['prompt_template'])\n",
    "    prompt_template = Path(fpath).read_text()\n",
    "    # create the payload for model inference\n",
    "    prompt = prompt_template.format(row['text']) \n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = generate_chapter_title(model_id, prompt) \n",
    "    # store the chapter text to return in the metrics dir\n",
    "    resp['chapter_text'] = row['text']\n",
    "    resp['file_name'] = row['file_name']\n",
    "    resp['chapter_id'] = row['chapter_id']\n",
    "    if row.get('title') is not None:\n",
    "        resp['original_title'] = row['title']\n",
    "    resp['prompt_token_count'] = int(resp['prompt_token_count'])\n",
    "    resp['completion_token_count'] = int(resp['completion_token_count'])\n",
    "    # calculate the input and output token price for all of the calls\n",
    "    resp['input_token_price'] = (resp['prompt_token_count']/1000) * model_info['input_tokens_pricing']\n",
    "    logger.info(f\"The price for {resp['prompt_token_count']} tokens for filename={row['file_name']} chapter={row['chapter_id']} is {resp['input_token_price']}\")\n",
    "    resp['output_token_pricing'] = (resp['completion_token_count']/1000) * model_info['output_tokens_pricing']\n",
    "    logger.info(f\"The price for {resp['completion_token_count']} tokens for filename={row['file_name']} chapter={row['chapter_id']} is {resp['output_token_pricing']}\")\n",
    "\n",
    "    dir_path = os.path.join(config['dir']['completions'], row['file_name'], model_id.replace(\":\", \"-\"))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fpath = os.path.join(dir_path, f\"chapter_{row['chapter_id']}.json\")\n",
    "    logger.info(f\"writing response={resp} to {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(resp, default=str, indent=2))\n",
    "    return resp\n",
    "\n",
    "@ray.remote\n",
    "def async_get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return get_inference(i, row, total, model_info)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_as_json = json.loads(df.to_json(orient='records'))\n",
    "n: int = config['parallel_inference_count']\n",
    "from typing import List\n",
    "resp_list: List = []\n",
    "for experiment in config['experiments']:\n",
    "    exp_name = experiment['name'] \n",
    "    model_list = experiment['model_list']\n",
    "    for model_info in model_list:\n",
    "        st = time.perf_counter()\n",
    "        logger.info(f\"------ running inference for {model_info['model']} -----\")\n",
    "        list_of_lists = [df_as_json[i * n:(i + 1) * n] for i in range((len(df_as_json) + n - 1) // n )]\n",
    "        logger.info(f\"split input list of size {len(df_as_json)} into {len(list_of_lists)} lists\")\n",
    "        for idx, l in enumerate(list_of_lists):\n",
    "            logger.info(f\"getting inference for list {idx+1}/{len(list_of_lists)}, size of list={len(l)} \")\n",
    "            resp_list.extend(ray.get([async_get_inference.remote(i+1, e, len(l), model_info) for i, e in enumerate(l)]))\n",
    "        elapsed_time = time.perf_counter() - st\n",
    "        logger.info(f\"------ model={model_info['model']} completed in {elapsed_time} ------ \")\n",
    "    time.sleep(config['sleep_in_seconds_between_models'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
