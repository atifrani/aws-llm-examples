{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for already chapterized data \n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "**Run this step if the data that is being used is already in chapterized format.**\n",
    "\n",
    "#### Prerequisites:\n",
    "\n",
    "1. Set up a `Python 3.11` conda environment with the packages in `requirements.txt` installed.\n",
    "\n",
    "1. Mention the correct `file type to process` in `dir` section given in the [`config.yml`](config.yml) file \n",
    "\n",
    "1. This notebook is tested on `json` files that contains an array of `already chapterized data` and the `original full meeting transcript`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this notebook if a JSON file already contains chapterized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from IPython.display import display, HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## load the config file\n",
    "CONFIG_FILE_PATH = \"config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_data(json_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes in a single json file, and inserts sections within a dataframe that contains information on\n",
    "    1. the name of the chapterized file \n",
    "    2. the id of the given chapter and the \n",
    "    3. content of the chapter in that file\n",
    "    \"\"\"\n",
    "    logger.info(f\"json_file=\\\"{json_file}\\\"\")\n",
    "    with open(json_file, 'r', encoding='utf-8') as file_object:\n",
    "        data = json.load(file_object)\n",
    "    logger.info(f\"chapter_data: {data}\")\n",
    "    file_name = os.path.basename(json_file)\n",
    "    logger.info(f\"Name of the file: {file_name}\")\n",
    "    df = pd.json_normalize(data['chapters'])\n",
    "    df.insert(0, 'file_name', file_name)\n",
    "    df.insert(1, 'chapter_id', df.index)\n",
    "    df.drop(columns=['start_ms'], inplace=True)\n",
    "    df.rename(columns={'transcript': 'text'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over files in that directory\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(config['dir']['processed'], exist_ok=True)\n",
    "df_list: List[pd.DataFrame] = []\n",
    "raw_files = glob.glob(os.path.join(config['dir']['raw'], f\"*.{config['dir']['file_type_to_process']}\"))\n",
    "logger.info(f\"there are {len(raw_files)} files to be processed \")\n",
    "\n",
    "if config['dir']['file_type_to_process'] == 'json':\n",
    "    df_list = list(map(lambda f: process_json_data(f), raw_files))\n",
    "    df = pd.concat(df_list)\n",
    "    fpath: str = os.path.join(config['dir']['processed'], config['dir']['chapterized_file'])\n",
    "    df.to_csv(fpath, encoding='utf-8', header='true', index=False)\n",
    "    logger.info(df.head())\n",
    "    logger.info(f\"Final processed dataframe of shape={df.shape} written to \\\"{fpath}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
