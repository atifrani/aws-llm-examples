{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate metrics & Run evaluations (ROUGE, COSINE, LLM acting as a judge in the loop)\n",
    "---\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. We will extract the titles generated as completions from the bedrock models (claude sonnet, llama, mistral), and load these into a CSV file \n",
    "\n",
    "1. Generate metrics on accuracy ([ROUGE-L](https://en.wikipedia.org/wiki/ROUGE_(metric)) and [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) scores), performance, token throughput, inference, etc.\n",
    "\n",
    "1. View all model completions to get a ***Vibe check*** on how each of the model performs. Next, have Claude Sonnet as a judge in the loop to go through each completion from multiple models, and decide which one best matches the human generated title. [Claude Sonnet](https://www.anthropic.com/claude) evaluates the most optimal model based on the [evaluation prompt](data/prompts/eval_template.txt) that is tuned into it. In this case, Sonnet acts as a judge to find the title that best captures the content of the meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd  \n",
    "from numpy import dot\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion ## support for text generation models on bedrock\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import Dict, Optional, List\n",
    "from bedrock_utils import get_bedrock_client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set a logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Ray Server that is used to run Asynchronous inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the config file: Contains model information, data directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the config file\n",
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Represents extracted all metric files\n",
    "fpath = os.path.join(config['dir']['completions'], \"**\", \"*\", \"*.json\")\n",
    "metric_files = glob.glob(fpath, recursive=True)\n",
    "logger.info(f\"there are {len(metric_files)} files in {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a simple CSV with metrics on title completions, chapters, and performance latency\n",
    "---\n",
    "\n",
    "1. This section of the notebook calculates metrics like title completions from each model in the config file for respective chapters, latency.\n",
    "\n",
    "1. The CSV also contains the original title that was given as a human generated title in the original data frame if any. If the human generated title is not provided, the data frame will not have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for f in metric_files:\n",
    "    metrics.append(json.loads(Path(f).read_text()))\n",
    "df = pd.DataFrame(metrics)\n",
    "df = df.drop(columns=['exception', 'prompt'])\n",
    "df = df.sort_values(by=['file_name', 'model_id', 'chapter_id'])\n",
    "df = df.rename(columns={'completion': 'chapter_title', 'time_taken_in_seconds': 'latency_seconds'})\n",
    "logger.info(f\"all metrics data is read into a dataframe of shape {df.shape}\")\n",
    "count = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Cosine versus ROUGE metrics for generated chapter titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_title(title):\n",
    "    \"\"\"\n",
    "    This function sanitizes the chapter titles that are generated. To add elements you want to remove from the chapter titles, modify the \n",
    "    'response_prefix_to_remove' in the config file\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        return title\n",
    "    suffixes_to_remove: List[str] = config['response_prefix_to_remove']\n",
    "    for response_to_remove in suffixes_to_remove:\n",
    "        title = title.replace(response_to_remove, \"\")\n",
    "    title = title.strip()\n",
    "    title = title.split(\"\\n\")[0]\n",
    "    return title\n",
    "df.chapter_title = df.chapter_title.map(sanitize_title)\n",
    "# view information about the type of data generated by the models, and other metrics below\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE & Cosine Similarity Scores for titles:\n",
    "---\n",
    "\n",
    "Here, the `amazon.titan-embed-text-v1` is used to get the embeddings of texts. To use a different embeddings model, change the `model` in the `embeddings_model_info` and modify this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "MAX_TEXT_LEN_FOR_EMBEDDING: int = config['embeddings_model_info']['max_text_len_for_embedding']\n",
    "bedrock: Optional[get_bedrock_client] = None\n",
    "\n",
    "def get_embedding(text: str, modelId: str=config['embeddings_model_info'].get('model'), accept: str='application/json', contentType: str='application/json'):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the chapter titles and original titles to generate cosine similarity measures\n",
    "    \"\"\"\n",
    "    global bedrock\n",
    "    if bedrock is None:\n",
    "        bedrock = get_bedrock_client()\n",
    "    body = json.dumps({\"inputText\": text[:MAX_TEXT_LEN_FOR_EMBEDDING]})\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    token_count = response_body.get('inputTextTokenCount')\n",
    "    return embedding, token_count\n",
    "\n",
    "def get_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between the chapter title generated from models, and the human generated title (if any)\n",
    "    \"\"\"\n",
    "    A,_ = get_embedding(text1)\n",
    "    B,_ = get_embedding(text2)\n",
    "    cosine = dot(A, B)/(norm(A)*norm(B))\n",
    "    return cosine\n",
    "\n",
    "def get_rouge_l_score(completion: str, golden: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the rouge-l score between the chapter title generated from models, and the human generated title (if any)\n",
    "    \"\"\"\n",
    "    rouge_metric_selection: str = config['embeddings_model_info']['rouge_metric_selection']\n",
    "    scorer = rouge_scorer.RougeScorer([rouge_metric_selection])\n",
    "    scores = scorer.score(golden, completion)\n",
    "    return round(scores[rouge_metric_selection].fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_titles(row):\n",
    "    \"\"\"\n",
    "    Generates the rouge and cosine similarity scores for chapter titles and original titles\n",
    "    \"\"\"\n",
    "    if (row.get('original_title') and row.get('chapter_title') is not None) and (pd.notna(row.get('original_title')) and pd.notna(row.get('chapter_title'))):\n",
    "        logger.info(f\"Chapter title: {row['chapter_title']}, Original title: {row['original_title']}\")\n",
    "        rouge_l_score = get_rouge_l_score(row['chapter_title'], row['original_title'])\n",
    "        cosine_sim = get_cosine_similarity(row['chapter_title'].lower(), row['original_title'].lower())\n",
    "        return pd.Series([rouge_l_score, cosine_sim])\n",
    "    else:\n",
    "        logger.info(f'ROUGE scores and Cosine similarity scores cannot be computed since original titles are not provided in the chapterized dataset')\n",
    "        rouge_l_score, cosine_sim = None, None\n",
    "\n",
    "if 'original_title' in df.columns:\n",
    "    df[['rouge_l_f1_score', 'cosine_similarity']] = df.apply(compare_titles, axis=1)\n",
    "else:\n",
    "    logger.info(f\"No evaluation metrics available since Golden titles are not provided in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of chapter titles generated by each of the model\n",
    "df_per_model_id_counts = df['model_id'].value_counts()\n",
    "df_per_model_id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dir = config['dir']['metrics']\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "# Construct the file path\n",
    "metrics_file_path = os.path.join(metrics_dir, config['dir']['metrics_file'])\n",
    "df.to_csv(metrics_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df.groupby('model_id').mean(numeric_only=True)\n",
    "if 'rouge_l_f1_score' and 'cosine_similarity' in df_summary.columns:\n",
    "    df_summary = df_summary.rename(columns={'rouge_l_f1_score': 'mean_rouge_l_f1_score', 'cosine_similarity': 'mean_cosine_similarity'})\n",
    "df_summary['p95_latency_seconds'] = df.groupby('model_id')['latency_seconds'].quantile(0.95)\n",
    "df_summary['avg_cost_per_txn'] = df_summary.input_token_price + df_summary.output_token_pricing\n",
    "df_summary['p95_cost_per_txn'] = df.groupby('model_id')['input_token_price'].quantile(0.95) + \\\n",
    "                                 df.groupby('model_id')['output_token_pricing'].quantile(0.95)\n",
    "df_summary.completion_token_count = df_summary.completion_token_count.astype(int)\n",
    "df_summary.prompt_token_count = df_summary.prompt_token_count.astype(int)\n",
    "df_summary['p95_completion_token_count'] = df.groupby('model_id')['completion_token_count'].quantile(0.95)\n",
    "df_summary['p95_prompt_token_count'] = df.groupby('model_id')['prompt_token_count'].quantile(0.95)\n",
    "df_summary = df_summary.drop(columns=['chapter_id'])\n",
    "# Reset the index to make 'model_id' a column\n",
    "df_summary = df_summary.reset_index()\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the long short view of the completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle if the title is given in the data frame, include it in the pivoted df, else exclude it\n",
    "if 'original_title'in df.columns:\n",
    "    index_cols = ['file_name', 'chapter_id', 'chapter_text', 'original_title']\n",
    "else:\n",
    "    index_cols = ['file_name', 'chapter_id', 'chapter_text']\n",
    "    \n",
    "df_pivoted = df.pivot_table(index=index_cols, columns='model_id', values='chapter_title', aggfunc='first')\n",
    "cols_other_than_index_cols = [f\"{c}_title\" for c in df_pivoted.columns if c not in index_cols]\n",
    "df_pivoted = df_pivoted.reset_index()\n",
    "df_pivoted.columns = index_cols + cols_other_than_index_cols\n",
    "df_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path\n",
    "movel_evals_fpath = os.path.join(metrics_dir, config['dir']['model_evals_file'])\n",
    "df_pivoted.to_csv(movel_evals_fpath, index=False)\n",
    "df_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(row, summary):\n",
    "    return summary.format(\n",
    "                model_id=row.name,\n",
    "                avg_latency=round(row['latency_seconds'], 4),\n",
    "                p95_latency=round(row['p95_latency_seconds'], 4),\n",
    "                avg_cost=round(10000 * row['avg_cost_per_txn'], 6),\n",
    "                p95_cost_per_txn=round(10000 * row['p95_cost_per_txn'], 6),\n",
    "                avg_prompt_token_count=row['prompt_token_count'],\n",
    "                p95_prompt_token_count=row['p95_prompt_token_count'],\n",
    "                avg_completion_token_count=row['completion_token_count'],\n",
    "                p95_completion_token_count=row['p95_completion_token_count'],\n",
    "                mean_rouge_l_score=('None' if row.get('mean_rouge_l_f1_score') is None else round(row['mean_rouge_l_f1_score'], 4)),\n",
    "                mean_cosine_similarity_score=('None, (no human generated title provided in the data)' if row.get('mean_cosine_similarity') is None else round(row['mean_cosine_similarity'], 4)),\n",
    "                count=int(row['count'])\n",
    "            )\n",
    "df_summary = pd.merge(left=df_summary, right=df_per_model_id_counts, on=\"model_id\", how=\"left\")\n",
    "\n",
    "df_summary['overall_report'] = df_summary.apply(lambda r: create_summary(r, config['report']['summary_text']), axis=1)\n",
    "df_summary = df_summary.round(6)\n",
    "summary_metrics_file_path = os.path.join(metrics_dir, config['dir']['summary_metrics_file'])\n",
    "df_summary = df_summary.sort_values(by=['mean_cosine_similarity', 'mean_rouge_l_f1_score'], ascending=False)\n",
    "df_summary.to_csv(summary_metrics_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the df_summary elements\n",
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Evaluation: Using LLM as a Judge in the loop\n",
    "---\n",
    "\n",
    "In this portion:\n",
    "\n",
    "1. Titles generated by each model are evaluated on relevance and meaning by [Claude](https://www.anthropic.com/news/claude-3-family) Sonnet/Your model of choice. Prompt for the model that acts as a judge in the loop can be viewed in: [eval_template.txt](data/prompts/eval_template.txt). Edit and review this prompt based on the use case and criteria for subjective evaluation.\n",
    "\n",
    "2. The role of the model acting as a judge it to compare the titles generated by each model to a human generated title (Aka ***golden title***). It provides information on the selected model, title, and an explanation of its selection, with an in depth analysis of comparison between other titles and why it chose the one it did. In this case, the model as a judge is prompted to ***capture the most relevant aspects of the meeting*** while generating a title.\n",
    "\n",
    "3. A final evaluation metric is calculated that shows the distribution of the selected models and their respective titles. This will give a judgement call of which model to use in production ready workloads.\n",
    "\n",
    "***Note: For more information on the use of having a Model act as a judge, view: https://huggingface.co/learn/cookbook/en/llm_judge***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # convert the model evaluation metrics stored as a df \n",
    "    model_eval_df = pd.read_csv(os.path.join(config['dir']['metrics'], config['dir']['model_evals_file']))  \n",
    "    logger.info(f\"Model eval file found with all model completions. Ready to evaluate responses...\")\n",
    "    model_eval_df.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model evaluation csv file not found in the directory. Error: {e}\")\n",
    "model_eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "\n",
    "Here, the [`evaluation prompt template`](data/prompts/eval_template.txt) is used by the LLM judge to evaluate different chapter titles and suggest the most suitable title based on the evaluation criteria mentioned in the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(row):\n",
    "    \"\"\"\n",
    "    This function evaluates the prompts by incorporating all of the titles generated by various bedrock models into the evaluation prompt template.\n",
    "    \"\"\"\n",
    "    # represents the eval template used by the model judge\n",
    "    eval_template: Optional[str] = None\n",
    "    processed_eval_template: Optional[str] = None\n",
    "    model_titles: List[str] = []\n",
    "    try:\n",
    "        # file path to the eval template\n",
    "        eval_template_path: str = os.path.join(config['dir']['prompts'], config['eval_model_info'].get('prompt_template'))\n",
    "        with open(eval_template_path, \"r\") as f:\n",
    "            eval_template = f.read()\n",
    "            logger.info(f\"evaluation prompt template recorded: {eval_template}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Evaluation template not found at {eval_template_path}\")\n",
    "    logger.info(f\"chapter_text: {row['chapter_text']}\")\n",
    "    logger.info(f\"original_title: {row['original_title']}\")\n",
    "    for column in row.index:\n",
    "        if column.endswith(\"_title\") and column != \"original_title\":\n",
    "            model_id = column.split(\"_title\")[0]\n",
    "            model_title = row[column]\n",
    "            model_titles.append(f\"\\n<{model_id}>\\n{model_title}\\n</{model_id}>\\n\")\n",
    "    processed_eval_template = eval_template.format(\n",
    "        chapter_text=row['chapter_text'], \n",
    "        original_title=row['original_title'],\n",
    "        model_titles=\"\\n\".join(model_titles)\n",
    "    )\n",
    "\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `evaluation prompt` as a column into a df with respective model and chapter titles to send into the Model for further evaluation in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_eval_df is not None:\n",
    "    model_eval_df['eval_prompt'] = model_eval_df.apply(lambda r: prepare_eval_prompts(r), axis=1)\n",
    "    logger.info(\"preparing the evaluation prompt templates for the LLM judge....\")\n",
    "else:\n",
    "    logger.error(f\"Model evaluation dataset is not available to process.\")\n",
    "model_eval_df_f_path = os.path.join(metrics_dir, config['dir']['processed_prompts_for_eval'])\n",
    "model_eval_df.to_csv(model_eval_df_f_path, index=False)\n",
    "model_eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LLM (Claude) as a judge in the loop to evaluate and narrow down the titles generated by different models of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_judge_json_evaluations(model_id: str, prompt: str):\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception = None,\n",
    "               prompt = prompt,\n",
    "               completion = None,\n",
    "               file_name = None,\n",
    "               original_title = None, \n",
    "               # initializing to 0 since none type throws an error later, this is used to calculate price per token input/output on ODT pricing\n",
    "               completion_token_count = 0,\n",
    "               # initializing to 0 since none type throws an error later\n",
    "               prompt_token_count=0,\n",
    "               input_token_price = None, \n",
    "               output_token_pricing = None,\n",
    "               model_id = model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    parameters = config['inference_parameters_for_explanations']\n",
    "    temperature = parameters.get('temperature', 0.1)\n",
    "    caching = parameters.get('caching', False)\n",
    "    max_tokens = parameters.get(\"max_tokens\", 500)\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        logger.info(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{ \"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=max_tokens,\n",
    "                              caching=caching)\n",
    "        \n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    \n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    # save all the responses from the model in a dictionary\n",
    "    resp: Dict = {}\n",
    "    print(f\"row={row}\")\n",
    "    logger.info(f\"row {i}/{total}, prompt_template={model_info['prompt_template']}, model_id={model_info['model']}\")\n",
    "    model_id = model_info['model']\n",
    "    # create the payload for model inference\n",
    "    prompt = row['eval_prompt']\n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = llm_judge_json_evaluations(model_id, prompt)\n",
    "    resp['original_title'] = row['original_title']\n",
    "    resp['file_name'] = row['file_name']\n",
    "    # calculate the input and output token price for all of the calls\n",
    "    resp['input_token_price'] = (resp['prompt_token_count']/1000) * model_info['input_tokens_pricing']\n",
    "    logger.info(f\"The price for {resp['prompt_token_count']} tokens for {model_id} for filename={row['file_name']} chapter={row['chapter_id']} is {resp['input_token_price']}\")\n",
    "    resp['output_token_pricing'] = (resp['completion_token_count']/1000) * model_info['output_tokens_pricing']\n",
    "    logger.info(f\"The price for {resp['completion_token_count']} tokens for {model_id} for filename={row['file_name']} chapter={row['chapter_id']} is {resp['output_token_pricing']}\")\n",
    "    dir_path = os.path.join(config['dir']['model_eval_completions'], row['file_name'], model_id.replace(\":\", \"-\"))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fpath = os.path.join(dir_path, f\"model_evaluation_{row['chapter_id']}.json\")\n",
    "    logger.info(f\"writing response={resp} to {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(resp, default=str, indent=2))\n",
    "    logger.info(f\"response {i}: {resp}\")\n",
    "    return resp\n",
    "\n",
    "@ray.remote\n",
    "def async_get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return get_inference(i, row, total, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_df = json.loads(model_eval_df.to_json(orient='records'))\n",
    "n: int = config['parallel_inference_count']\n",
    "from typing import List\n",
    "resp_list: List = []\n",
    "model_list = config['eval_model_info']\n",
    "st = time.perf_counter()\n",
    "logger.info(f\"------ running inference for {model_list.get('model')} -----\")\n",
    "list_of_lists = [model_eval_df[i * n:(i + 1) * n] for i in range((len(model_eval_df) + n - 1) // n )]\n",
    "logger.info(f\"split input list of size {len(model_eval_df)} into {len(list_of_lists)} lists\")\n",
    "for idx, l in enumerate(list_of_lists):\n",
    "    logger.info(f\"getting inference for list {idx+1}/{len(list_of_lists)}, size of list={len(l)} \")\n",
    "    resp_list.extend(ray.get([async_get_inference.remote(i+1, e, len(l), model_list) for i, e in enumerate(l)]))\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"------ model={model_list.get('model')} completed in {elapsed_time} ------ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all evaluations from the model evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Represents extracted all metric files\n",
    "fpath_evaluated_files = os.path.join(config['dir']['model_eval_completions'], \"**\", \"*\", \"*.json\")\n",
    "eval_metric_files = glob.glob(fpath_evaluated_files, recursive=True)\n",
    "logger.info(f\"there are {len(eval_metric_files)} evaluated files by {config['eval_model_info']['model']} LLM judge in {fpath_evaluated_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_responses = []\n",
    "for f in eval_metric_files:\n",
    "    with open(f, 'r') as file:\n",
    "        model_evaluation_responses.append(json.loads(file.read()))\n",
    "# results_df will contain the evaluation responses, including the completion and the model id\n",
    "results_df = pd.DataFrame(model_evaluation_responses)\n",
    "results_df = results_df.drop(columns=['exception', 'prompt', 'file_name'])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_eval_json(data):\n",
    "    \"\"\"\n",
    "    This function is to take in json data, and clean it, assign the selected title as outputted by the model evaluator\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = json.loads(data.replace('\\\\', '\\\\\\\\'))\n",
    "        return pd.Series({\n",
    "            'best_match_title': json_data['best_match_title'],\n",
    "            'selected_model': json_data['selected_model'],\n",
    "            'explanation': json_data['explanation'],\n",
    "        })\n",
    "    except json.JSONDecodeError:\n",
    "        return pd.Series({\n",
    "            'best_match_title': None,\n",
    "            'selected_model': None,\n",
    "            'explanation': None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results_df = results_df['completion'].apply(clean_model_eval_json)\n",
    "# removing any unnecessary characters from the selected_model if any\n",
    "new_results_df['selected_model'] = new_results_df['selected_model'].str.replace(r'<[^>]+>', '', regex=True)\n",
    "# here we split the elements of the selected_model column using the tidy split function\n",
    "new_exploded_df = tidy_split(new_results_df, 'selected_model', sep=',')\n",
    "new_results_df['chapter_title'] = results_df['original_title']\n",
    "new_results_df['input_token_price'] = results_df['input_token_price']\n",
    "new_results_df['output_token_price'] = results_df['output_token_pricing']\n",
    "new_results_df = new_results_df.reindex(columns=['chapter_title', 'best_match_title', 'selected_model', 'explanation', 'input_token_price', 'output_token_price'])\n",
    "logger.info(f\"All evaluation data is read into a dataframe of shape {results_df.shape}\")\n",
    "processed_prompts_for_eval_path = os.path.join(metrics_dir, config['dir']['filtered_titles_for_eval'])\n",
    "new_results_df.to_csv(processed_prompts_for_eval_path, index=False)\n",
    "# display the selected title, model explanation and the respective golden title in a side by side view\n",
    "new_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the percentage of each model selection and reset the index\n",
    "new_exploded_df['selected_model'] = new_exploded_df['selected_model'].map(lambda x: x.strip())\n",
    "model_percentage_df = new_exploded_df['selected_model'].value_counts(normalize=True).reset_index()\n",
    "model_percentage_df['proportion'] *= 100\n",
    "model_distribution_fpath = os.path.join(metrics_dir, config['dir']['model_distribution'])\n",
    "model_percentage_df.to_csv(model_distribution_fpath, index=False)\n",
    "model_percentage_df.rename(columns = {'selected_model':'model_id'}, inplace = True)\n",
    "model_percentage_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the most frequently selected model\n",
    "most_selected_index = model_percentage_df.proportion.idxmax()\n",
    "report_template: str = config['report']['model_recommendation']\n",
    "report: str = report_template.format(\n",
    "                count=new_results_df.best_match_title.count(),\n",
    "                model_id=model_percentage_df.iloc[most_selected_index]['model_id'],\n",
    "                percentage_of_occurrence=model_percentage_df.proportion.max(), \n",
    "                total_evaluation_cost=round((new_results_df.input_token_price.sum() + new_results_df.output_token_price.sum()), 4))\n",
    "result_data = {'model_recommendation': [report]}\n",
    "results_summary_df = pd.DataFrame(result_data)\n",
    "recommended_model_fpath = os.path.join(metrics_dir, config['dir']['final_report'])\n",
    "# Saving to CSV\n",
    "results_summary_df.to_csv(recommended_model_fpath, index=False)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_summary, model_percentage_df, on='model_id', how='left')\n",
    "merged_df.rename(columns={'proportion': 'LLM_as_a_judge_pick_rate'}, inplace=True)\n",
    "merged_df['LLM_as_a_judge_pick_rate'] = merged_df['LLM_as_a_judge_pick_rate'].fillna(\"not available\")\n",
    "merged_df['mean_rouge_l_f1_score'] = merged_df['mean_rouge_l_f1_score'].fillna(\"not available\")\n",
    "merged_df['mean_cosine_similarity'] = merged_df['mean_cosine_similarity'].fillna(\"not available\")\n",
    "eval_report_template = config['report']['eval_report_explanation']\n",
    "\n",
    "# Calculate the evaluation report for each row for the mean cosine, rouge and llm as a judge pick rate\n",
    "merged_df['eval_report'] = merged_df.apply(lambda row: eval_report_template.format(\n",
    "    rouge_score=row['mean_rouge_l_f1_score'],\n",
    "    cosine_score=row['mean_cosine_similarity'],\n",
    "    llm_as_a_judge=row['LLM_as_a_judge_pick_rate']\n",
    "), axis=1)\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "cols = merged_df.columns.tolist()\n",
    "idx = cols.index('mean_cosine_similarity')\n",
    "cols.insert(idx + 1, cols.pop(cols.index('LLM_as_a_judge_pick_rate')))\n",
    "cols.insert(idx + 2, cols.pop(cols.index('eval_report')))\n",
    "merged_df = merged_df[cols]\n",
    "merged_df.to_csv(summary_metrics_file_path, index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explanation results from llm as a judge\n",
    "new_results_df = new_results_df.loc[:, ~new_results_df.columns.duplicated()]\n",
    "new_results_df = new_results_df.rename(columns={'selected_model': 'model_id'})\n",
    "new_results_df = pd.merge(new_results_df, merged_df[['model_id', 'eval_report']], on='model_id', how='left')\n",
    "new_results_df = new_results_df.rename(columns={'model_id': 'selected_model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the report right next to the explanation\n",
    "cols = new_results_df.columns.tolist()\n",
    "explanation_idx = cols.index('explanation')\n",
    "cols.insert(explanation_idx + 1, cols.pop(cols.index('eval_report')))\n",
    "new_results_df = new_results_df[cols]\n",
    "new_results_df.to_csv(processed_prompts_for_eval_path, index=False)\n",
    "new_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Recommended LLM based on a combined score of `Subjective` and `Quantitative` evaluation using `LLM as a judge`, `ROUGE` and `Cosine Similarity` metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 in the normalized pick rate\n",
    "merged_df['LLM_as_a_judge_pick_rate'].replace('not available', 0, inplace=True)\n",
    "merged_df['mean_rouge_l_f1_score'].replace('not available', 0, inplace=True)\n",
    "merged_df['mean_cosine_similarity'].replace('not available', 0, inplace=True)\n",
    "merged_df['LLM_as_a_judge_pick_rate'] = merged_df['LLM_as_a_judge_pick_rate'] / 100\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_llm_judge_model = merged_df.sort_values(by='LLM_as_a_judge_pick_rate', ascending=False).iloc[0]['model_id']\n",
    "best_llm_judge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rouge_score_model = merged_df.sort_values(by='mean_rouge_l_f1_score', ascending=False).iloc[0]['model_id']\n",
    "best_rouge_score_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cosine_model = merged_df.sort_values(by='mean_cosine_similarity', ascending=False).iloc[0]['model_id']\n",
    "best_cosine_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_llm_judge_model_value = merged_df.sort_values(by='LLM_as_a_judge_pick_rate', ascending=False).iloc[0]['LLM_as_a_judge_pick_rate']\n",
    "best_llm_judge_model_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_model(df) -> str:\n",
    "    \"\"\"\n",
    "    This function computes the recommended model based on the three evaluation criteria.\n",
    "    If a model has the highest score for all three criteria, then it becomes the best model agreed by all three.\n",
    "    If not, then it is checked for the combination of the rest of the two criteria. If none of the cases satisfy,\n",
    "    then a best recommended model is returned for each of the evaluation criteria.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        evaluation_report: Optional[str] = None\n",
    "        # model with the highest score using LLM as a judge eval\n",
    "        best_llm_judge_model = df.sort_values(by='LLM_as_a_judge_pick_rate', ascending=False).iloc[0]['model_id']\n",
    "        best_llm_judge_model_value = df.sort_values(by='LLM_as_a_judge_pick_rate', ascending=False).iloc[0]['LLM_as_a_judge_pick_rate']\n",
    "        # model with the highest score using the ROUGE f1 score\n",
    "        best_rouge_score_model = df.sort_values(by='mean_rouge_l_f1_score', ascending=False).iloc[0]['model_id']\n",
    "        best_rouge_score_model_value = df.sort_values(by='mean_rouge_l_f1_score', ascending=False).iloc[0]['mean_rouge_l_f1_score']\n",
    "        # model with the highest score using the Cosine Similarity score\n",
    "        best_cosine_model = df.sort_values(by='mean_cosine_similarity', ascending=False).iloc[0]['model_id']\n",
    "        best_cosine_model_value = df.sort_values(by='mean_cosine_similarity', ascending=False).iloc[0]['mean_cosine_similarity']\n",
    "\n",
    "        # check if all three models that are selected on the three criteria are the same\n",
    "        if best_llm_judge_model == best_rouge_score_model == best_cosine_model:\n",
    "            evaluation_report = (\n",
    "                f\"As per all three evaluation criteria, '{best_llm_judge_model}' is the best recommended model for your workload \"\n",
    "                f\"based on the LLM as a judge pick rate of {best_llm_judge_model_value*100}%, Cosine Similarity of {best_cosine_model_value} and ROUGE score of {best_rouge_score_model_value}.\"\n",
    "            )\n",
    "        # Check combinations of any two criteria permutations\n",
    "        elif best_llm_judge_model == best_rouge_score_model:\n",
    "            evaluation_report = (\n",
    "                f\"As per the two evaluation criteria, '{best_llm_judge_model}' is the best recommended model for your workload \"\n",
    "                f\"based on the LLM as a judge pick rate of {best_llm_judge_model_value*100}%, and ROUGE score of {best_rouge_score_model_value}.\"\n",
    "            )\n",
    "        elif best_llm_judge_model == best_cosine_model:\n",
    "            evaluation_report = (\n",
    "                f\"As per the two evaluation criteria, '{best_llm_judge_model}' is the best recommended model for your workload \"\n",
    "                f\"based on the LLM as a judge pick rate of {best_llm_judge_model_value*100}%, and Cosine Similarity score of {best_cosine_model_value}.\"\n",
    "            )\n",
    "        elif best_rouge_score_model == best_cosine_model:\n",
    "            evaluation_report = (\n",
    "                f\"As per the two evaluation criteria, '{best_rouge_score_model}' is the best recommended model for your workload \"\n",
    "                f\"based on the Cosine Similarity of {best_cosine_model_value} and ROUGE score of {best_rouge_score_model_value}.\"\n",
    "            )\n",
    "        # If none of the combinations match, recommend based on each individual criterion\n",
    "        else:\n",
    "            evaluation_report = (\n",
    "                f\"Based on each evaluation criteria, the following models are best recommended. \"\n",
    "                f\"LLM as a judge selects {best_llm_judge_model} as the best recommended model. \"\n",
    "                f\"Cosine Similarity score selects {best_cosine_model} as the best recommended model. \"\n",
    "                f\"ROUGE score selects {best_rouge_score_model} as the best recommended model.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The best recommended model could not be provided: {e}\")\n",
    "        evaluation_report: Optional[str] = None\n",
    "    return evaluation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the overall model recommendation based on the three evaluation criteria\n",
    "recommendation = recommend_model(merged_df)\n",
    "# Save the overall model evaluation recommendation to a csv\n",
    "overall_eval_report_fpath: str = os.path.join(config['dir']['metrics'], config['dir']['overall_eval_report'])\n",
    "overall_eval_data = {'overall_eval_recommendation': [recommendation]}\n",
    "overall_eval_df = pd.DataFrame(overall_eval_data)\n",
    "overall_eval_df.to_csv(overall_eval_report_fpath, index=False)\n",
    "print(recommendation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
