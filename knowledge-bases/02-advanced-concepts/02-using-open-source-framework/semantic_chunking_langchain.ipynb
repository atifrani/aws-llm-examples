{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, you'll learn:\n",
    "1. [What is semantic chunking](#31-what-is-semantic-chunking)\n",
    "2. [How to use semantic chunking in LangChain to pre-process documents](#32-implementation-using-langchain)\n",
    "3. [Put the pre-processed files to Amazon S3 and ingest into Knowledge Bases for Amazon Bedrock](#33-ingest-files-to-the-knowledge-base)\n",
    "4. [Test the knowledge base](#34-test-the-knowledge-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 What is semantic chunking?\n",
    "Semantic chunking is an experimental method which splits text based on their semantic similarity â€“ similar sentences stay in the same chunk. Briefly speaking, the method performs the following:\n",
    "1. Create embeddings for (group of) sentences\n",
    "2. Compare the similarity between adjacent groups\n",
    "3. Set a similarity threshold\n",
    "4. Join the adjacent groups which have similarity above the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Data ingestion flow\n",
    "![semantic chunking with Knowledge Bases for Amazon Bedrock](images/semantic-chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Use the same embedding model across the whole data flow, including\n",
    "* Document chunking (using open source library)\n",
    "* Document embedding (managed by Knowledge Bases for Amazon Bedrock)\n",
    "* Document retrieval (managed by Knowledge Bases for Amazon Bedrock)\n",
    "\n",
    "In this workshop, you'll use **Amazon Titan Embeddings G1 - Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implementation using LangChain\n",
    "Reference: https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Read sample data\n",
    "In this section you'll use the Amazon 2023 letter to shareholders as sample data. Feel free to use your own PDF file by changing the `file_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PDF file content\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader\n",
    "\n",
    "file_url = 'https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf'\n",
    "\n",
    "file = requests.get(file_url)\n",
    "file_io = BytesIO(file.content)\n",
    "reader = PdfReader(file_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the whole doc\n",
    "# Perform semantic split later on\n",
    "num_pages = len(reader.pages)\n",
    "text = \"\"\n",
    "for i in range(num_pages):\n",
    "    text += reader.pages[i].extract_text() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the text\n",
    "# len(text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Perform semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AWS credentials\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "model_id = 'amazon.titan-embed-text-v1'\n",
    "embeddings = BedrockEmbeddings(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text semantically\n",
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "\n",
    "breakpoint_percentile = 95  # type: percentile\n",
    "# breakpoint_sd = 3  # type: standard_deviation\n",
    "# breakpoint_iqr = 1.5  # type: interquartile\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type = 'percentile',\n",
    "    breakpoint_threshold_amount = breakpoint_percentile\n",
    ")\n",
    "split_texts = semantic_chunker.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have 17 chunks from the 11-page document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore thd splits\n",
    "for split_text in split_texts:\n",
    "    print(len(split_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split 5 is too short (11 characters). Let's see what's in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_texts[4])\n",
    "print('---')\n",
    "print(split_texts[5])\n",
    "print('---')\n",
    "print(split_texts[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore different threshold types and values. See if you can create better splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ingest files to the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section assumes that you've the following resources ready:\n",
    "* A S3 bucket to store the split text files\n",
    "* An Amazon Bedrock knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Put split files to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Target S3 bucket and prefix\n",
    "s3_bucket = '<your bucket name>'  # replace this with your S3 bucket name\n",
    "s3_prefix = 'semantic_chunk_workshop'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Write each split text to a separate file in S3\n",
    "for i, split_text in enumerate(split_texts):\n",
    "    s3_key = f'{s3_prefix}/{i}.txt'\n",
    "    print(f'Writing to s3://{s3_bucket}/{s3_key}')\n",
    "    s3_client.put_object(Body=split_text, Bucket=s3_bucket, Key=s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Synchronize documents to Knowledge Bases for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.1 Using AWS Console\n",
    "1. Go to the [Knowledge Bases for Amazon Bedrock console](https://console.aws.amazon.com/bedrock/home#/knowledge-bases)\n",
    "2. Select your knowledge base, and click **Edit**\n",
    "3. In the **Data source** section, click **Add**\n",
    "4. Change the following and click **Add**\n",
    "    1. S3 URI: s3://\\<your bucket name\\>/semantic_chunk_workshop/\n",
    "    2. Expand **Advanced settings**, and select **No chunking**\n",
    "    ![Disable chunking in data source](img/semantic-chunking-data-source-config.png)\n",
    "5. Once completed, click **Sync** to ingest data from S3 into the knowledge base\n",
    "6. Check the **Sync history** and you'll see that 17 source files added (or other numbers if you use a different data or chunking settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.2 Using boto3\n",
    "If you prefer to create a data source programmatically, run the following cells. Otherwise, skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Target knowledge base\n",
    "kb_id = '<knowledge base id>'  # Replace with your knowledge base ID\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data source\n",
    "data_source_config = {\n",
    "    \"s3Configuration\": {\n",
    "        \"bucketArn\": f\"arn:aws:s3:::{s3_bucket}\",\n",
    "        \"inclusionPrefixes\": [f\"{s3_prefix}/\"]},\n",
    "    \"type\": \"S3\"\n",
    "}\n",
    "description = 'Data source for semantic chunking workshop'\n",
    "name = 'semantic-chunking-workshop'\n",
    "vectorIngestionConfiguration = {\n",
    "    'chunkingConfiguration': {'chunkingStrategy': 'NONE'}\n",
    "}\n",
    "\n",
    "response = bedrock_agent_client.create_data_source(\n",
    "    dataSourceConfiguration=data_source_config,\n",
    "    description=description,\n",
    "    knowledgeBaseId=kb_id,\n",
    "    name=name,\n",
    "    vectorIngestionConfiguration=vectorIngestionConfiguration\n",
    ")\n",
    "\n",
    "ds_id = response.get(\"dataSource\").get(\"dataSourceId\")  # Data source ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize documents to knowledge base\n",
    "response = bedrock_agent_client.start_ingestion_job(\n",
    "    dataSourceId=ds_id,\n",
    "    knowledgeBaseId=kb_id,\n",
    ")\n",
    "\n",
    "job_id = response.get(\"ingestionJob\").get(\"ingestionJobId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop until the job has completed or failed\n",
    "import time\n",
    "\n",
    "for i in range(10):\n",
    "    time.sleep(30)  # Wait for 30 seconds before checking the status of the job\n",
    "    response = bedrock_agent_client.get_ingestion_job(\n",
    "        dataSourceId=ds_id,\n",
    "        ingestionJobId=job_id,\n",
    "        knowledgeBaseId=kb_id,\n",
    "    )\n",
    "    status = response.get(\"ingestionJob\").get(\"status\")\n",
    "    if status != 'IN_PROGRESS':\n",
    "        print(f'Ingestion Job {job_id} is {status}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Using AWS Console\n",
    "1. Go to the [Knowledge Bases for Amazon Bedrock console](https://console.aws.amazon.com/bedrock/home#/knowledge-bases)\n",
    "2. Select your knowledge base, and click **Test knowledge base**\n",
    "3. Make sure **Generate responses** is enabled\n",
    "4. Click **Select model**\n",
    "5. Choose **Claude 3 Haiku** or any other models and click **Apply**\n",
    "6. Enter your question and click **Run**\n",
    "7. Check the response, expand **source details** to check the references\n",
    "\n",
    "For example:\n",
    "![Sample response with semantic chunking](img/semantic-chunking-test-retrieve-generate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Target knowledge base\n",
    "kb_id = '<knowledge base id>'  # Replace with your knowledge base ID\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents and generate response\n",
    "region = boto3.Session().region_name\n",
    "model_arn = f'arn:aws:bedrock:{region}::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "# Helper function\n",
    "def ask(question, session_id=None):\n",
    "    # Construct the config\n",
    "    config = {\n",
    "        \"input\": {\n",
    "            \"text\": question\n",
    "        },\n",
    "        \"retrieveAndGenerateConfiguration\": {\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "            },\n",
    "            \"type\": \"KNOWLEDGE_BASE\"\n",
    "        }\n",
    "    }\n",
    "    # Add session_id if exists\n",
    "    if session_id:\n",
    "        config[\"sessionId\"] = session_id\n",
    "    # Invoke the knowledge base API\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        **config\n",
    "    )\n",
    "    session_id = response.get(\"sessionId\")\n",
    "    output = response.get(\"output\")\n",
    "    citations = response.get(\"citations\")\n",
    "    return session_id, output, citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the first question in a new session\n",
    "question = 'What is project Kuiper?'\n",
    "session_id, output, citations = ask(question)\n",
    "print(output.get('text'))\n",
    "# print(citations)\n",
    "# print(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow-up question\n",
    "question = 'Tell me more about it.'\n",
    "session_id, output, citations = ask(question, session_id)\n",
    "print(output.get('text'))\n",
    "# print(citations)\n",
    "# print(session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
