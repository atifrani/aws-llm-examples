{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Knowledge Bases for Amazon Bedrock to decide right Chunking Size based on your dataset for optimizing RAG application\n",
    "\n",
    "This notebook provides sample code for building multiple knowledge bases for Amazon Bedrock based on different chunk sizes.\n",
    "\n",
    "Repeat the following steps for each chunk size (you want to evaluate):\n",
    "\n",
    "- Create execution role for Knowledge Bases for Amazon Bedrock with necessary policies for accessing data from S3 and writing embeddings into vector store (OpenSearchServerless).\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Download documents (or point to your document S3 location)\n",
    "- Create knowledge base for Amazon Bedrock \n",
    "- Create a data source within knowledge base which will connect to Amazon S3\n",
    "- Once the data is available in the Bedrock Knowledge Bases with different chunk size, we'll evaluate the text chunks retreived for refernce QA pairs from these knowledge bases for faithfulness, correctness, and relevancy metrics using LlamaIndex. \n",
    "- Using these metrics we can decide for RIGHT chunk size for our RAG based application. \n",
    "\n",
    "Finally, based on the evaluation results, a question answering application with RIGHT chunk strategy can be built using the Amazon Bedrock APIs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "### Dataset\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Please ignore error messages related to pip's dependency resolver.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.33.2 --force-reinstall --quiet\n",
    "%pip install botocore==1.33.2 --force-reinstall --quiet\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U retrying==1.3.4\n",
    "\n",
    "%pip install langchain==0.0.342 --force-reinstall --quiet\n",
    "%pip install llama-index==0.9.3.post1 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from retrying import retry\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting boto3 clients for required AWS services\n",
    "sts_client = boto3.client('sts')\n",
    "iam_client = boto3.client('iam')\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
    "aoss_client = boto3.client('opensearchserverless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region_name, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate random suffix for unique vector store, vector index, and KB name\n",
    "suffix = random.randrange(200, 900)\n",
    "# Bucket name  and prefix for KB files \n",
    "bucket_name = f\"chunk-evaluation-{region_name}-{account_id}\" # replace it with your bucket name.\n",
    "s3_prefix = 'shareholder-letters'\n",
    "\n",
    "# Create S3 bucket for uploading the data\n",
    "s3bucket = s3_client.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Create OSS policies and collection\n",
    "First of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "base_index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "\n",
    "# create IAM role\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_name = bedrock_kb_execution_role['Role']['RoleName']\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_kb_execution_role_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(collection)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 -  Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to s3 in a prefix\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file), bucket_name, s3_prefix + '/' + file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Chunking Evaluation Setup\n",
    "\n",
    "### OSS Configurations\n",
    "We'll set-up multiple KBs with various chunk sizes, and create multiple vector indexes for storing Embeddings for corresponding chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Base Configurations\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": base_index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    \"inclusionPrefixes\":[s3_prefix] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these configurations as input to the `create_e2e_kb` method (defined below), which will set-up the end-to-end Knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 -  Create seperate Knowledge base for each chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create separate Knowledge base for each chunk size. This function will create e2e Knowledge base by following following steps:\n",
    "\n",
    "- Create Knowledge bases and OpenSearchServerless collection/index based on the input configuration\n",
    "- Create Datasource for each Knowledge base\n",
    "- Create Ingestion Job for ingesting data into each Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to setup end-to-end knowledge base as mentioned above\n",
    "\n",
    "def create_e2e_kb(kb_name, kb_description, bucket_name, chunk_size, overlapPercentage, oss_client, base_index_name):\n",
    "    \n",
    "    index_name = base_index_name + '-' + str(chunk_size)\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    time.sleep(20)\n",
    "    \n",
    "    # Set the parameters value\n",
    "    opensearchServerlessConfiguration['vectorIndexName'] = index_name\n",
    "    chunkingStrategyConfiguration['fixedSizeChunkingConfiguration']['maxTokens']= chunk_size\n",
    "    chunkingStrategyConfiguration['fixedSizeChunkingConfiguration']['overlapPercentage']= overlapPercentage\n",
    "    \n",
    "   \n",
    "    kb_name = f\"{kb_name}-{suffix}-{chunk_size}\"\n",
    "    description = f\"{kb_description}-{chunk_size}\"\n",
    "    \n",
    "    # Create a KnowledgeBase\n",
    "    kb_response=''\n",
    "    \n",
    "    try:\n",
    "        # @retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "        create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "                name = kb_name,\n",
    "                description = description,\n",
    "                roleArn = roleArn,\n",
    "                knowledgeBaseConfiguration = {\n",
    "                    \"type\": \"VECTOR\",\n",
    "                    \"vectorKnowledgeBaseConfiguration\": {\n",
    "                        \"embeddingModelArn\": embeddingModelArn\n",
    "                    }\n",
    "                },\n",
    "                storageConfiguration = {\n",
    "                    \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "                    \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "                }\n",
    "            )\n",
    "\n",
    "        kb_response= create_kb_response[\"knowledgeBase\"]\n",
    "        # Get KnowledgeBase \n",
    "        print(f\"KB ID: {kb_response['knowledgeBaseId']} KB Name: {kb_response['name']}\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"{err=}, {type(err)=}\")\n",
    "    \n",
    "    # Get KnowledgeBase \n",
    "    get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb_response['knowledgeBaseId'])\n",
    "        \n",
    "    # Create a DataSource in KnowledgeBase \n",
    "    create_ds_response = bedrock_agent_client.create_data_source(\n",
    "        name = kb_name,\n",
    "        description = description,\n",
    "        knowledgeBaseId = kb_response['knowledgeBaseId'],\n",
    "        dataSourceConfiguration = {\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\":s3Configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration = {\n",
    "            \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "        }\n",
    "    )\n",
    "    ds = create_ds_response[\"dataSource\"]\n",
    "    \n",
    "    # Get DataSource \n",
    "    bedrock_agent_client.get_data_source(knowledgeBaseId = kb_response['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "    \n",
    "    # Start an ingestion job\n",
    "    start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb_response['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    " \n",
    "    kb_id = kb_response[\"knowledgeBaseId\"]\n",
    "    \n",
    "    return kb_name, kb_id, index_name, job\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define variables for creating KnowledgeBase\n",
    "\n",
    "base_kb_name = f\"amazon-shareholder-letters-knowledge-base\"\n",
    "kb_description = \"Amazon shareholder letter knowledge base.\"\n",
    "overlapPercentage = 20\n",
    "\n",
    "kb_oss_index_dict={}\n",
    "job_list =[]\n",
    "\n",
    "# Define chunksize you want to evaluate\n",
    "chunkSizes = [300, 512, 700]\n",
    "data = []\n",
    "\n",
    "# Create KB for each chunk size\n",
    "for chunk_size in chunkSizes:\n",
    "    kb_name, kb_id, index_name, job = create_e2e_kb(base_kb_name, kb_description, bucket_name, chunk_size, overlapPercentage, oss_client, base_index_name)\n",
    "    kb_oss_index_dict[chunk_size] = {'kb_id':kb_id, 'kb_name':kb_name, 'index_name':index_name}\n",
    "    job_list.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait while Ingestion job is running\n",
    "# Get job status\n",
    "for job in job_list:\n",
    "    while(job['status']!='COMPLETE' ):\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "          knowledgeBaseId = job_list[-1]['knowledgeBaseId'],\n",
    "            dataSourceId = job_list[-1][\"dataSourceId\"],\n",
    "            ingestionJobId = job_list[-1][\"ingestionJobId\"]\n",
    "          )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        time.sleep(10)\n",
    "    # pp.pprint(job)\n",
    "    # time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(kb_oss_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store kb_oss_index_dict, so we can retrieve it while CleanUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store kb_oss_index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Chunking evaluation setup\n",
    "\n",
    "    - Define Retrieve function - to get text chunks\n",
    "    - Define Prompt template specific to the model\n",
    "    - Define Refernce QA pair\n",
    "    - Define function to evaluate text chunks for faithfulness, correctness, and relevancy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve functions that calls Retrieve API\n",
    "\n",
    "Define a retrieve function that calls the `Retreive API` provided by Knowledge bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom\n",
    "workﬂows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(query, kbId, numberOfResults=5):\n",
    "    return bedrock_agent_runtime_client.retrieve(\n",
    "        retrievalQuery= {\n",
    "            'text': query\n",
    "        },\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration= {\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': numberOfResults\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template - specific to the model to personalize responses \n",
    "\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the `Retrieve API` responses from above as a part of the `{context_str}` in the prompt for the model to refer to, along with the user `query`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context_str}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query_str}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\"\"\"\n",
    "titan_prompt = PromptTemplate(template=PROMPT_TEMPLATE, \n",
    "                               input_variables=[\"context_str\",\"query_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch the text chunks from the RetrieveAPI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch context from the response\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference QA pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_question_answer_pair = [(\"How many days has Amazon asked employees to come to work in office?\",\n",
    "                          \"Amazon has asked corporate employees to come back to office at least three days a week beginning May 2022.\"),\n",
    "                         (\"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "                          \"AWS had a 29% year-over-year ('YoY') revenue in 2022 on $62B revenue base.\"),\n",
    "                         (\"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\",\n",
    "                          \"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"),\n",
    "                         (\"Which was the first inference chip launched by AWS according to the passage?\",\n",
    "                          \"AWS launched their first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"),\n",
    "                         (\"According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\",\n",
    "                          \"Amazon's annual revenue increased from $245B in 2019 to $434B in 2022.\"\n",
    "                          )\n",
    "                          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate Chunk size for RAG workflow\n",
    "\n",
    "For our evaluation, we will use the Retreive API provided by Knowledge bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the scores of the retrievals.\n",
    "\n",
    "Next, we'll evaluate the relevancy of text chunks corresponding to the original prompt using LLaMaIndex relevancy metrics, and compare it against a threshold value.\n",
    "\n",
    "- if relevancy score for retreived text chunks doesn't pass the threshold, then original prompt won't be passed to large language model (LLM) to generate the response.\n",
    "    \n",
    "- otherwise, we will use the text chunks being generated and augment it with the original prompt and pass it through the LLM using prompt engineering patterns.\n",
    "\n",
    "        - Finally we will evaluate the generated responses using metrics such as faithfulness, correctness, and relevancy metrics. For evaluation, we will use Anthropic Claude v2 model, and for response generation we'll use amazon.titan-text-express-v1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import LangChainLLM\n",
    "from langchain.llms import Bedrock\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from llama_index.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, CorrectnessEvaluator\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "def evaluate_chunk_size_kb(chunk_size, qa_pairs, kb_id, vector_index, relevancy_threshold):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by amazon.titan-text-lite-v1 for a given chunk size.\n",
    "    We'll use 'amazon.titan-embed-text-v1' for embedding and 'anthropic.claude-v2' to evaluate the response\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "    qa_pairs (list): List of QA tuples\n",
    "    kb_id (str) : KnowledgeBase ID under evaluation\n",
    "    vector_index (str): vector index associated with KnowledgeBase\n",
    "    relevancy_threshold (float) : Threshold value for Relevancy\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Configurations\n",
    "    llm_parameters = {\n",
    "    \"maxTokenCount\":2000,\n",
    "    \"stopSequences\":[],\n",
    "    \"temperature\":0,\n",
    "    \"topP\":0.9\n",
    "    }\n",
    "\n",
    "    model_kwargs_claude = {\n",
    "        \"temperature\": 0,\n",
    "        \"top_k\": 10,\n",
    "        \"max_tokens_to_sample\": 3000\n",
    "    }\n",
    "\n",
    "   \n",
    "    # 1. define bedrock model to generate and evaluate the responses\n",
    "    llm = Bedrock(model_id = \"amazon.titan-text-express-v1\",\n",
    "              model_kwargs=llm_parameters,\n",
    "              client = bedrock_runtime_client,)\n",
    "    \n",
    "    # 2. define bedrock model to evaluate the responses\n",
    "    llm_claude = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "              model_kwargs=model_kwargs_claude,\n",
    "              client = bedrock_runtime_client,)\n",
    "    \n",
    "    # 3. Define the embed model to be used - \n",
    "    embed_model = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1')\n",
    "    \n",
    "    # Finally, get vector index\n",
    "    vector_index = vector_index\n",
    "    kb_id = kb_id\n",
    "    \n",
    "    # Establishing evaluators - Pass two parameters llm and embed model in the service context\n",
    "    serviceContextLLM = ServiceContext.from_defaults(llm = llm_claude, embed_model=embed_model)\n",
    "    set_global_service_context(service_context)\n",
    "    faithfulnessLLM = FaithfulnessEvaluator(service_context=serviceContextLLM)\n",
    "    relevancyLLM = RelevancyEvaluator(service_context=serviceContextLLM)\n",
    "    CorrectnessLLM = CorrectnessEvaluator(service_context=serviceContextLLM)\n",
    "    \n",
    "    \n",
    "    # Define variables\n",
    "    relevancy_threshold = relevancy_threshold # Relevancy threshold\n",
    "    \n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "    total_correctness = []\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    average_response_time = 0\n",
    "    average_faithfulness = 0\n",
    "    average_relevancy = 0\n",
    "    avereage_correctness = 0\n",
    "    \n",
    "    \n",
    "    # Check for the avg relevancy score\n",
    "    '''\n",
    "    If avg relevency score is under 'relevancy_threshold', we won't pass the context to LLM to\n",
    "    generate the response. This will avoid unneccessary cost for LLM inference.\n",
    "    '''\n",
    "    relevancy_score = 0\n",
    "    avg_relevancy_score = 0\n",
    "    \n",
    "    for question, reference_answer in qa_pairs:\n",
    "        result = retrieve(question, kb_id, 5)\n",
    "        retrievalResults = result['retrievalResults']\n",
    "        # get context\n",
    "        contexts = get_contexts(retrievalResults=retrievalResults)\n",
    "        relevancy_result = relevancyLLM.evaluate(query=question,\n",
    "                                                 response=reference_answer, \n",
    "                                                  contexts=contexts)\n",
    "        relevancy_score += relevancy_result.score\n",
    "     \n",
    "    avg_relevancy_score = relevancy_score/len(qa_pairs)\n",
    "      \n",
    "    eval_qa_num = 0\n",
    "   \n",
    "    # if 'avg_relevancy_score' passes the threshold -  then calculate the metrics\n",
    "    if avg_relevancy_score > relevancy_threshold:\n",
    "       \n",
    "        # Iterate over each question in eval_question_answer_pair to compute metrics.\n",
    "        # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "        for question, reference_answer in qa_pairs:\n",
    "            print(\"\\t Question:\", question)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # retrieve matching documents\n",
    "            result = retrieve(question, kb_id, 5)\n",
    "            retrievalResults = result['retrievalResults']\n",
    "            contexts = get_contexts(retrievalResults=retrievalResults)\n",
    "            #call LLM with updated context and question.\n",
    "            prompt = titan_prompt.format(context_str=contexts, \n",
    "                                     query_str=question)\n",
    "            response_vector = llm(prompt)\n",
    "            # generated_answer = str(response_vector)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                \n",
    "                \n",
    "                faithfulness_result = faithfulnessLLM.evaluate(query=question,\n",
    "                                                              response=response_vector, \n",
    "                                                              contexts=contexts)\n",
    "\n",
    "                relevancy_result = relevancyLLM.evaluate(query=question,\n",
    "                                                              response=response_vector, \n",
    "                                                              contexts=contexts)\n",
    "            \n",
    "                correctness_result = CorrectnessLLM.evaluate(\n",
    "                                                            query=question,\n",
    "                                                            response=response_vector,\n",
    "                                                            reference=reference_answer)\n",
    "                \n",
    "                print(f\"\\t Eval Result -> elapsed_time: {elapsed_time}, faithfulness: {faithfulness_result.passing}, relevancy:{relevancy_result.score}, correctness: {correctness_result.score}\" )\n",
    "                print(\"\\t Response:\", response_vector)\n",
    "                total_response_time += elapsed_time\n",
    "                total_faithfulness += faithfulness_result.passing\n",
    "                total_relevancy += relevancy_result.score\n",
    "                total_correctness.append(correctness_result.score)\n",
    "                eval_qa_num += 1      \n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"\\tAn error occured while evaluating the question!!!\")\n",
    "                print(e)\n",
    "                total_response_time += elapsed_time\n",
    "                total_faithfulness = 0\n",
    "                total_relevancy = 0\n",
    "                total_correctness.append(0.0)\n",
    "                print(f\"\\t Eval Result -> elapsed_time: {elapsed_time}, faithfulness: err, relevancy: err, correctness: err\" )\n",
    "\n",
    "        # Calculate average metrics\n",
    "        average_response_time = total_response_time / eval_qa_num\n",
    "        average_faithfulness = total_faithfulness / eval_qa_num\n",
    "        average_relevancy = total_relevancy / eval_qa_num\n",
    "        avereage_correctness = sum(total_correctness)/ eval_qa_num\n",
    "                      \n",
    "    # if 'avg_relevancy_score' doesn't passe the threshold -  then don't send to LLM\n",
    "    else:\n",
    "        print(\"\\t Avg relevancy score was below threshold, so Context not sent to the LLM\")\n",
    "        average_response_time = 0\n",
    "        average_faithfulness = 0\n",
    "        average_relevancy = relevancy_score\n",
    "        avereage_correctness = 0\n",
    "        \n",
    "    \n",
    "    # metrics_score ={}\n",
    "    metrics_score = {\n",
    "                    'average_relevancy' : f'{average_relevancy:.2f}',\n",
    "                    'avg_response_time' : f'{average_response_time:.2f}',\n",
    "                    'average_faithfulness' : f'{average_faithfulness:.2f}',\n",
    "                    'avereage_correctness' : f'{avereage_correctness:.2f}',\n",
    "                    # 'detail_eval_df': evals_df\n",
    "                    } \n",
    "    print(\"\\t Evaluation result: \", metrics_score)\n",
    "\n",
    "    return  metrics_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chunk_sizes = list(kb_oss_index_dict.keys())\n",
    "relevancy_threshold = 0.6\n",
    "data = []\n",
    "\n",
    "base_kb_name = f\"amazon-shareholder-letters-knowledge-base\"\n",
    "base_index_name = f\"bedrock-sample-index\"\n",
    "\n",
    "for chunk_size in chunk_sizes[0:3]:\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"Chunk Size - {chunk_size} evaluation : Initiating...\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"In progress....\")\n",
    "    kb_id = kb_oss_index_dict[chunk_size]['kb_id']\n",
    "    vector_index = kb_oss_index_dict[chunk_size]['index_name']\n",
    "    \n",
    "    metrics_score= evaluate_chunk_size_kb(chunk_size, eval_question_answer_pair, kb_id, vector_index, relevancy_threshold)\n",
    "    print(f\"Chunk size {chunk_size} - Average Relevancy: {metrics_score['average_relevancy']}, Average Response time: {metrics_score['avg_response_time']}, Average Faithfulness: {metrics_score['average_faithfulness']},  Average Correctness: {metrics_score['avereage_correctness']}\")\n",
    "    data.append({\n",
    "            'kb_id':kb_id, \n",
    "            'vector_index': vector_index,  \n",
    "            'Chunk Size': chunk_size, \n",
    "            'Average Relevancy': metrics_score['average_relevancy'],\n",
    "            'Average Response Time': metrics_score['avg_response_time'],\n",
    "            'Average Faithfulness': metrics_score['average_faithfulness'],\n",
    "            'Average Correctness': metrics_score['avereage_correctness']} )\n",
    "    print(\"Done!!\\n\")\n",
    "    # detail_eval_df = metrics_score['detail_eval_df']\n",
    "    time.sleep(20)\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric type\n",
    "col_to_convert =['Chunk Size', 'Average Response Time','Average Faithfulness', 'Average Relevancy', 'Average Correctness']\n",
    "df[col_to_convert] = df[col_to_convert].apply(pd.to_numeric)\n",
    "\n",
    "#Plot metrics\n",
    "ax = df.plot(\n",
    "        x='Chunk Size', \n",
    "        y=['Average Response Time','Average Faithfulness', 'Average Relevancy', 'Average Correctness'], \n",
    "        kind='bar', \n",
    "        figsize=(9,6))\n",
    "\n",
    "\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.bar_label(ax.containers[1])\n",
    "ax.bar_label(ax.containers[2])\n",
    "ax.bar_label(ax.containers[3])\n",
    "\n",
    "ax.legend( bbox_to_anchor =(1 ,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test example\n",
    "\n",
    "Let's evaluate the response for each chunk size to for a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[300]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[512]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[700]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index and S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty and delete S3 Bucket\n",
    "\n",
    "objects = s3_client.list_objects(Bucket=bucket_name)  \n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=obj['Key']) \n",
    "s3_client.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Delete all KnowledgeBases\n",
    "\n",
    "for item in job_list:\n",
    "    bedrock_agent_client.delete_data_source(dataSourceId = item[\"dataSourceId\"], knowledgeBaseId=item['knowledgeBaseId'])\n",
    "    bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=item['knowledgeBaseId'])\n",
    "    \n",
    "# delete all indexes\n",
    "for key in kb_oss_index_dict:\n",
    "    oss_client.indices.delete(index=kb_oss_index_dict[key]['index_name'])\n",
    "\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "## DELETE ROLE\n",
    "try: \n",
    "    # Get the list of attached policies\n",
    "    attached_policies = iam_client.list_attached_role_policies(RoleName=bedrock_kb_execution_role_name)\n",
    "\n",
    "    # Detach each policy\n",
    "    for policy in attached_policies['AttachedPolicies']:\n",
    "        iam_client.detach_role_policy(RoleName=bedrock_kb_execution_role_name, PolicyArn=policy['PolicyArn'])\n",
    "        iam_client.delete_policy( PolicyArn=policy['PolicyArn'])\n",
    "\n",
    "    # Delete role\n",
    "    iam_client.delete_role(RoleName=bedrock_kb_execution_role_name)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"couldn't delete role\", bedrock_kb_execution_role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedrock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
